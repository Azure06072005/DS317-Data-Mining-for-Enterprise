{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9768477,"sourceType":"datasetVersion","datasetId":5906743},{"sourceId":13431267,"sourceType":"datasetVersion","datasetId":8524856},{"sourceId":13438003,"sourceType":"datasetVersion","datasetId":8529421}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/trakiet/duachuot?scriptVersionId=271178219\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:40:26.596282Z","iopub.execute_input":"2025-10-27T04:40:26.596974Z","iopub.status.idle":"2025-10-27T04:40:27.878666Z","shell.execute_reply.started":"2025-10-27T04:40:26.596948Z","shell.execute_reply":"2025-10-27T04:40:27.877667Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/mooccubex-dataset/course-teacher.txt\n/kaggle/input/mooccubex-dataset/course.json\n/kaggle/input/mooccubex-dataset/paper.json\n/kaggle/input/mooccubex-dataset/reply.json\n/kaggle/input/mooccubex-dataset/concept-comment.txt\n/kaggle/input/mooccubex-dataset/user-video.json\n/kaggle/input/mooccubex-dataset/course-comment.txt\n/kaggle/input/mooccubex-dataset/concept-problem.txt\n/kaggle/input/mooccubex-dataset/user-reply.txt\n/kaggle/input/mooccubex-dataset/school.json\n/kaggle/input/mooccubex-dataset/translated_reply.json\n/kaggle/input/mooccubex-dataset/video.json\n/kaggle/input/mooccubex-dataset/concept-video.txt\n/kaggle/input/mooccubex-dataset/other.json\n/kaggle/input/mooccubex-dataset/course-school.txt\n/kaggle/input/mooccubex-dataset/problem.json\n/kaggle/input/mooccubex-dataset/user-comment.txt\n/kaggle/input/mooccubex-dataset/user.json\n/kaggle/input/mooccubex-dataset/exercise-problem.txt\n/kaggle/input/mooccubex-dataset/concept-paper.txt\n/kaggle/input/mooccubex-dataset/translated_comment.json\n/kaggle/input/mooccubex-dataset/reply-reply.txt\n/kaggle/input/mooccubex-dataset/course-field.json\n/kaggle/input/mooccubex-dataset/video_id-ccid.txt\n/kaggle/input/mooccubex-dataset/concept-reply.json\n/kaggle/input/mooccubex-dataset/user-xiaomu.json\n/kaggle/input/mooccubex-dataset/user-problem.json\n/kaggle/input/mooccubex-dataset/concept.json\n/kaggle/input/mooccubex-dataset/comment-reply.txt\n/kaggle/input/mooccubex-dataset/teacher.json\n/kaggle/input/mooccubex-dataset/concept-other.txt\n/kaggle/input/df-user-video-final/part-00000-8eb8369b-ec66-434a-9e3c-608414a06b1f-c000.csv\n/kaggle/input/user-course-v4/user_course_v4.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"BASE_DIR = '/kaggle/input/mooccubex-dataset/'\ndf_user_course_path = '/kaggle/input/user-course-v4/user_course_v4.csv'\ndf_user_video_final_path = '/kaggle/input/df-user-video-final/part-00000-8eb8369b-ec66-434a-9e3c-608414a06b1f-c000.csv'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:40:27.879686Z","iopub.execute_input":"2025-10-27T04:40:27.880104Z","iopub.status.idle":"2025-10-27T04:40:27.885147Z","shell.execute_reply.started":"2025-10-27T04:40:27.880075Z","shell.execute_reply":"2025-10-27T04:40:27.883866Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import pyspark\nfrom pyspark.sql import SparkSession\nimport warnings\nfrom pyspark.sql.types import *\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import IntegerType\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom os import truncate\nfrom pyspark.sql.functions import explode, col, sum, from_unixtime, hour, dayofweek, count, round, when, isnull, split, avg, regexp_replace\nwarnings.filterwarnings(\"ignore\")\n\nspark = SparkSession.builder \\\n  .appName(\"reply_user_video_analysis\") \\\n  .master(\"local[*]\") \\\n  .config(\"spark.some.config.option\", \"some-value\") \\\n  .getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:40:27.887045Z","iopub.execute_input":"2025-10-27T04:40:27.887337Z","iopub.status.idle":"2025-10-27T04:40:37.351309Z","shell.execute_reply.started":"2025-10-27T04:40:27.887315Z","shell.execute_reply":"2025-10-27T04:40:37.350235Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/10/27 04:40:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!ls {BASE_DIR}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:40:37.353015Z","iopub.execute_input":"2025-10-27T04:40:37.353864Z","iopub.status.idle":"2025-10-27T04:40:37.492684Z","shell.execute_reply.started":"2025-10-27T04:40:37.353769Z","shell.execute_reply":"2025-10-27T04:40:37.491544Z"}},"outputs":[{"name":"stdout","text":"comment-reply.txt    course-school.txt\t      translated_reply.json\nconcept-comment.txt  course-teacher.txt       user-comment.txt\nconcept.json\t     exercise-problem.txt     user.json\nconcept-other.txt    other.json\t\t      user-problem.json\nconcept-paper.txt    paper.json\t\t      user-reply.txt\nconcept-problem.txt  problem.json\t      user-video.json\nconcept-reply.json   reply.json\t\t      user-xiaomu.json\nconcept-video.txt    reply-reply.txt\t      video_id-ccid.txt\ncourse-comment.txt   school.json\t      video.json\ncourse-field.json    teacher.json\ncourse.json\t     translated_comment.json\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df_user_video=spark.read.json(BASE_DIR + \"/user-video.json\")\n# df_user_video.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-27T04:40:37.493962Z","iopub.execute_input":"2025-10-27T04:40:37.494317Z"}},"outputs":[{"name":"stderr","text":"[Stage 0:==============>                                           (6 + 4) / 24]\r","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"df_user_video.printSchema()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.functions import col, explode, from_json, expr\nfrom pyspark.sql.types import StructType, StructField, StringType, ArrayType\n\n# Giải nén `seq` để mỗi dòng có một `video_id`\ndf_flatten = df_user_video.withColumn(\"seq\", explode(col(\"seq\")))\n\n# Giải nén tiếp `segment` để mỗi dòng là một đoạn xem video\ndf_flatten = df_flatten.withColumn(\"segment\", explode(col(\"seq.segment\")))\n\n# Chọn các cột cần thiết\ndf_flatten = df_flatten.select(\n    col(\"user_id\"),\n    col(\"seq.video_id\").alias(\"video_id\"),\n    col(\"segment.start_point\"),\n    col(\"segment.end_point\"),\n    col(\"segment.speed\"),\n    col(\"segment.local_start_time\")\n)\n# df_flatten.show(truncate=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.functions import from_unixtime\n\ndf_flatten = df_flatten.withColumn(\"local_start_time\", from_unixtime(\"local_start_time\", \"yyyy-MM-dd HH:mm:ss\"))\n# df_flatten.show(truncate=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_flatten = df_flatten.filter(col(\"start_point\") <= col(\"end_point\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_flatten.count()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**video_duration:** Tổng thời gian dài nhất mà video đã được xem (last_end_time). Đây là thời gian tối đa của video mà người dùng đã từng xem.<br>\n**rewind_time:**\tTổng thời gian người dùng tua lại video (khi start_point của đoạn hiện tại nhỏ hơn end_point của đoạn trước đó).<br>\n**fast_forward_time:**\tTổng thời gian người dùng tua tới trong video (khi start_point của đoạn hiện tại lớn hơn end_point của đoạn trước đó).<br>\n**first_local_start_time:**\tThời điểm đầu tiên mà người dùng bắt đầu xem video (local_start_time của lần xem đầu tiên).<br>\n**actual_watch_time:**\tTổng thời gian thực tế mà người dùng đã xem video (tổng các đoạn end_point - start_point). Không tính việc video bị tua nhanh hay chậm.<br>\n**weighted_watch_time:**\tTổng thời gian xem có tính tốc độ (lấy mỗi đoạn thời gian thực tế chia cho tốc độ tương ứng, rồi cộng lại). <br>Công thức: \\\nweighted_watch_time = **sum(**(end_point - start_point) / speed**)**  . \t<br>\nGiá trị này phản ánh thời gian video đã chạy nếu xem với tốc độ gốc (1.0x).\n**average_speed:**\tTốc độ xem trung bình của người dùng trên video. <br>\nCông thức: *actual_watch_time / weighted_watch_time.* <br>\nNếu bằng 1.0, người dùng xem với tốc độ gốc. Nếu lớn hơn 1.0, người dùng xem nhanh hơn. Nếu nhỏ hơn 1.0, người dùng xem chậm lại.","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import col, min, max, count, sum, lag, first, when, countDistinct\n\nfrom pyspark.sql.window import Window\n\n# Tạo cửa sổ để so sánh với điểm trước đó của user trên cùng video\nwindow_spec = Window.partitionBy(\"user_id\", \"video_id\").orderBy(\"local_start_time\")\n\n# Tạo cột prev_end_point để so sánh với điểm trước đó của video\ndf_lagged = df_flatten.withColumn(\"prev_end_point\", lag(\"end_point\").over(window_spec)) \\\n                      .withColumn(\"prev_speed\", lag(\"speed\").over(window_spec)) \\\n                      .withColumn(\"prev_start_time\", lag(\"local_start_time\").over(window_spec))\n# Tính toán các giá trị mong muốn\ndf_user_video_final = df_lagged.groupBy(\"user_id\", \"video_id\").agg(\n    round(max(\"end_point\"), 2).alias(\"max_watch_point\"),      # Tổng thời gian của video (giá trị lớn nhất của end_point)\n    sum(\n        when(col(\"prev_end_point\") > col(\"start_point\"),\n             round(col(\"prev_end_point\") - col(\"start_point\"), 2))  # Nếu prev_end > start -> tua lại\n        .otherwise(0)\n    ).alias(\"rewind_time\"),\n    sum(\n        when(col(\"start_point\") > col(\"prev_end_point\"),\n             round(col(\"start_point\") - col(\"prev_end_point\"), 2))  # Nếu start > prev_end -> tua tới\n        .otherwise(0)\n    ).alias(\"fast_forward_time\"),\n    first(\"local_start_time\").alias(\"first_local_start_time\"),  # Lần đầu tiên xem video\n    round(sum(col(\"end_point\") - col(\"start_point\")), 2).alias(\"actual_watch_time\"),  # Tổng thời gian thực tế đã xem\n    round(sum((col(\"end_point\") - col(\"start_point\")) / col(\"speed\")), 2).alias(\"weighted_watch_time\"),  # Tổng thời gian thực / speed\n\n    # --- CÁC ĐẶC TRƯNG MỚI ĐỀ XUẤT ---\n    \n    # 1. Tần suất và Phiên xem: Ý nghĩa: Đếm tổng số \"đoạn\" log (rows) cho mỗi (user, video). Một con số cao có thể cho thấy người dùng xem video nhiều lần, hoặc tạm dừng/tua rất nhiều.\n    count(\"*\").alias(\"interaction_count\"),\n    (sum(when(col(\"local_start_time\").cast(\"long\") - col(\"prev_start_time\").cast(\"long\") > 1800, 1).otherwise(0)) + 1).alias(\"session_count\"),\n    # 2. Hành vi Tốc độ: Đếm số phiên xem riêng biệt. Ta có thể định nghĩa một \"phiên mới\" là khi thời gian giữa hai lần tương tác liên tiếp vượt quá một ngưỡng (ví dụ: 30 phút).\n    sum(when(col(\"speed\") != col(\"prev_speed\"), 1).otherwise(0)).alias(\"speed_change_count\"),\n    countDistinct(\"speed\").alias(\"distinct_speeds_count\"),\n\n    # 3. Yếu tố Thời gian: Khoảng thời gian từ lần tương tác đầu tiên đến lần tương tác cuối cùng của video đó. Nó cho biết video này đã được xem trong một khoảng thời gian dài (ví dụ: vài ngày) hay xem liền một mạch.\n    hour(first(\"local_start_time\")).alias(\"start_hour\"),\n    dayofweek(first(\"local_start_time\")).alias(\"start_dayofweek\")\n)\n\n# Tính tốc độ xem trung bình\n# 3. Tính các đặc trưng phái sinh (Post-Aggregation)\ndf_user_video_final = df_user_video_final.withColumn(\n    \"average_speed\", \n    round(col(\"actual_watch_time\") / col(\"weighted_watch_time\"), 2)\n)\n# df_user_video_final.show(truncate=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_user_video_final.count()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"uv_null_ratio = df_user_video_final.select([\n    (F.count(F.when(F.col(c).isNull(), 1))).alias(c) for c in df_user_video_final.columns\n])\n# uv_null_ratio.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_course = spark.read.json(BASE_DIR + '/course.json')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_video_course_map = (\n    df_course\n    .withColumn(\n        \"video_resource\",\n        F.filter(F.col(\"resource\"), lambda x: x[\"resource_id\"].startswith(\"V_\"))\n    )\n    .withColumn(\"video_struct\", F.explode_outer(F.col(\"video_resource\")))\n    .select(\n        F.col(\"id\").alias(\"course_id\"),\n        F.col(\"video_struct.resource_id\").alias(\"video_id\")\n    )\n)\n\n# df_video_course_map.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_video = spark.read.json(BASE_DIR + \"/video.json\")\n# df_video.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql.functions import array_max\ndf_video = (\n    df_video\n    .select(\n        \"ccid\", \n        array_max(\"end\").alias(\"video_duration\")\n    )\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_video.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_video_id_ccid = spark.read.text(BASE_DIR + '/video_id-ccid.txt')\n# Parse mapping file\ndf_video_id_ccid_v2 = df_video_id_ccid.select(\n    split(col(\"value\"), \"\\t\")[0].alias(\"video_id\"),\n    split(col(\"value\"), \"\\t\")[1].alias(\"ccid\")\n)\n# df_video_id_ccid_v2.show(5, truncate=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"video_info = df_video_id_ccid_v2.join(df_video, on=['ccid'])\n# video_info.show(truncate=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"course_video_info = df_video_course_map.join(video_info, 'video_id', 'left')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"course_video_info = course_video_info.drop('ccid')\n# course_video_info.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# course_video_info.count()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_user_course = spark.read.csv(df_user_course_path, header=True, inferSchema=True)\n# df_user_course.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# count = df_user_course_clean.select([\n#     (F.count(F.when(F.col(c).isNull(), 1))).alias(c) for c in df_user_course_clean.columns\n# ])\n# count.show(truncate=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_user_course_group = df_user_course.join(course_video_info, 'course_id', 'left')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# df_user_course_group.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyspark.sql import functions as F\n\n# --- 1. Tối ưu hóa việc tạo df_with_stats ---\n\n# Bước 1.1: Tạo DataFrame thống kê riêng bằng groupBy\n# Đây là cách tính toán hiệu quả hơn Window function\ndf_course_stats = df_user_course_group.groupBy(\"course_id\", \"user_id\").agg(\n    F.count(\"video_id\").alias(\"course_total_videos\"),\n    F.round(F.sum(\"video_duration\"), 2).alias(\"course_total_duration_secs\")\n)\n\n# Bước 1.2: Join các giá trị thống kê này trở lại DataFrame gốc\n# Chúng ta đã có df_with_stats mà không cần dùng Window\ndf_with_stats_fixed = df_user_course_group.join(\n    df_course_stats,\n    on=[\"course_id\", \"user_id\"],\n    how=\"left\"\n)\n\n# --- 2. Thực hiện phép JOIN cuối cùng của bạn (nguyên nhân gây lỗi) ---\n# Bây giờ phép join này sẽ chạy trên df_with_stats_fixed đã được tối ưu\ndf_final = df_with_stats_fixed.join(\n    df_user_video_final, \n    ['user_id', 'video_id'], \n    'left'\n)\n\n# --- 3. (Khuyến nghị) Xử lý các giá trị NULL như ở phần trước ---\n# Các video được đăng ký nhưng \"chưa xem\" sẽ có giá trị NULL\n# Ta điền 0 vào các cột hành vi\nbehavior_cols = [\n    'max_watch_point', 'rewind_time', 'fast_forward_time', 'actual_watch_time',\n    'weighted_watch_time', 'interaction_count', 'session_count', 'speed_change_count',\n    'distinct_speeds_count', 'average_speed'\n]\n\ndf_final = df_final.fillna(0, subset=behavior_cols)\n\n# Gán tốc độ 1.0 cho các video chưa xem\ndf_final = df_final.withColumn(\n    \"average_speed\",\n    F.when(F.col(\"actual_watch_time\") == 0, 1.0) \n    .otherwise(F.col(\"average_speed\"))\n)\n\n\n# --- 4. Hiển thị kết quả (Bây giờ sẽ chạy được) ---\nprint(\"Đã join thành công, đang hiển thị kết quả:\")\ndf_final.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check = df_with_stats.filter(F.col('video_duration') > 0)\ncheck.show(200, truncate=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Thử Nghiệm ","metadata":{}},{"cell_type":"code","source":"# from pyspark.sql import functions as F\n# from pyspark.sql.types import ArrayType, StringType, TimestampType\n\n# # 1. Chuẩn bị Dữ liệu Video (Không đổi)\n# df_video_durations = video_info_cleaned.select(\n#     F.col(\"video_id\"),\n#     F.col(\"video_duration\")\n# ).distinct()\n\n# # 2. Chuẩn bị Dữ liệu Khóa học (Không đổi)\n# df_course_video_details = df_video_course_map_clean.join(\n#     df_video_durations,\n#     \"video_id\",\n#     \"left\"\n# )\n# df_course_stats = df_course_video_details.groupBy(\"course_id\").agg(\n#     F.collect_list(\"video_id\").alias(\"course_video_list\"),\n#     F.sum(\"video_duration\").alias(\"total_video_duration\"),\n#     F.count(\"video_id\").alias(\"num_videos_in_course\")\n# )\n\n# # 3. Chuẩn bị Dữ liệu Xem của Người dùng-Khóa học\n# df_user_watch_by_course = df_user_video_final.join(\n#     df_video_course_map_clean,\n#     \"video_id\",\n#     \"inner\"\n# )\n\n# # --- BẮT ĐẦU TINH CHỈNH 1 ---\n# # Đổi tên 'video_duration' (từ log) để tránh xung đột\n# # Cột này (từ user-video.json) đáng tin cậy hơn để tính tỷ lệ hoàn thành sự kiện\n# df_user_watch_events = df_user_watch_by_course.withColumnRenamed(\"video_duration\", \"log_video_duration\")\n\n# # Join với duration \"chính thức\" (từ video.json)\n# df_user_watch_events = df_user_watch_events.join(\n#     df_video_durations,\n#     \"video_id\",\n#     \"left\"\n# )\n\n# # Tính 'video_completion_rate_event' DÙNG 'log_video_duration' (đáng tin cậy hơn)\n# df_user_watch_events = df_user_watch_events.withColumn(\n#     \"video_completion_rate_event\",\n#     F.when(F.col(\"log_video_duration\") > 0, F.col(\"actual_watch_time\") / F.col(\"log_video_duration\"))\n#      .otherwise(0)\n# ).withColumn(\n#     \"is_rewatch\",\n#     F.when(F.col(\"rewind_time\") > 0, 1).otherwise(0)\n# )\n# # --- KẾT THÚC TINH CHỈNH 1 ---\n\n\n# # Tổng hợp hành vi của người dùng (Không đổi)\n# df_user_course_watch_stats = df_user_watch_events.groupBy(\"user_id\", \"course_id\").agg(\n#     F.collect_list(\"video_id\").alias(\"user_video_list_with_duplicates\"),\n#     F.sum(\"actual_watch_time\").alias(\"total_watched_duration\"),\n#     F.avg(\"actual_watch_time\").alias(\"avg_watched_duration\"),\n#     F.avg(\"average_speed\").alias(\"avg_speed\"),\n#     F.count(\"video_id\").alias(\"num_sessions\"),\n#     F.countDistinct(\"video_id\").alias(\"num_videos_watched\"),\n#     F.sum(\"is_rewatch\").alias(\"num_rewatches\"),\n#     F.avg(\"video_completion_rate_event\").alias(\"avg_completion_rate_per_video\"), # (Giờ sẽ tính đúng)\n#     F.min(\"first_local_start_time\").alias(\"first_local_start_time\")\n# ).withColumn(\n#     \"user_video_list\", F.array_distinct(\"user_video_list_with_duplicates\")\n# ).drop(\"user_video_list_with_duplicates\")\n\n\n# # 4. Gộp và Tính toán (Bước cuối cùng)\n# df_final_agg = df_user_course_clean.join(\n#     df_course_stats,\n#     \"course_id\",\n#     \"left\"\n# ).join(\n#     df_user_course_watch_stats,\n#     [\"user_id\", \"course_id\"],\n#     \"left\"\n# )\n\n# # Xử lý giá trị NULL (Không đổi)\n# numeric_fill_cols = {\n#     \"total_video_duration\": 0.0,\n#     \"num_videos_in_course\": 0,\n#     \"total_watched_duration\": 0.0,\n#     \"avg_watched_duration\": 0.0,\n#     \"avg_speed\": 0.0,\n#     \"num_sessions\": 0,\n#     \"num_videos_watched\": 0,\n#     \"num_rewatches\": 0,\n#     \"avg_completion_rate_per_video\": 0.0\n# }\n# df_final_agg = df_final_agg.fillna(numeric_fill_cols)\n# df_final_agg = df_final_agg.withColumn(\n#     \"course_video_list\",\n#     F.when(F.col(\"course_video_list\").isNull(), F.array().cast(ArrayType(StringType())))\n#      .otherwise(F.col(\"course_video_list\"))\n# ).withColumn(\n#     \"user_video_list\",\n#     F.when(F.col(\"user_video_list\").isNull(), F.array().cast(ArrayType(StringType())))\n#      .otherwise(F.col(\"user_video_list\"))\n# )\n\n# # Tính toán các chỉ số cuối cùng\n# df_final_output_raw = df_final_agg.withColumn(\n#     \"num_common_videos\",\n#     F.col(\"num_videos_watched\")\n# ).withColumn(\n#     \"estimated_watched_in_course\",\n#     F.col(\"total_watched_duration\")\n# ).withColumn(\n#     \"completion_rate_raw\", # Đặt tên thô\n#     F.when(F.col(\"total_video_duration\") > 0, F.col(\"total_watched_duration\") / F.col(\"total_video_duration\"))\n#      .otherwise(0.0)\n# ).withColumn(\n#     \"unique_videos_ratio_raw\", # Đặt tên thô\n#     F.when(F.col(\"num_videos_in_course\") > 0, F.col(\"num_videos_watched\") / F.col(\"num_videos_in_course\"))\n#      .otherwise(0.0)\n# ).withColumn(\n#     \"rewatch_ratio_original\",\n#     F.when(F.col(\"num_sessions\") > 0, F.col(\"num_rewatches\") / F.col(\"num_sessions\"))\n#      .otherwise(0.0)\n# ).withColumn(\n#     \"rewatch_sessions_ratio\",\n#     F.when(\n#         (F.col(\"num_sessions\") > 0) & (F.col(\"num_sessions\") > F.col(\"num_videos_watched\")),\n#         (F.col(\"num_sessions\") - F.col(\"num_videos_watched\")) / F.col(\"num_sessions\")\n#     ).otherwise(0.0)\n# ).withColumn(\n#     \"video_activity_ratio\",\n#     F.when(F.col(\"num_videos_in_course\") > 0, F.col(\"num_sessions\") / F.col(\"num_videos_in_course\"))\n#      .otherwise(0.0)\n# ).withColumn(\n#     \"days_from_enroll_to_first_watch\",\n#     F.when(\n#         F.col(\"first_local_start_time\").isNotNull() & F.col(\"enroll_time\").isNotNull(),\n#         F.datediff(\n#             F.col(\"first_local_start_time\").cast(\"date\"),\n#             F.col(\"enroll_time\").cast(\"date\")\n#         )\n#     ).otherwise(None)\n# )\n\n# # --- BẮT ĐẦU TINH CHỈNH 2: CHUẨN HÓA (CAPPING) ---\n# # Giới hạn các giá trị ngoại lệ (outliers) do lỗi dữ liệu\n# MAX_COMPLETION = 2.0 # Giới hạn tỷ lệ hoàn thành ở 200% (cho phép xem lại 1 lần)\n# MAX_RATIO = 1.0       # Giới hạn tỷ lệ video ở 100%\n\n# df_final_output = df_final_output_raw.withColumn(\n#     \"completion_rate\",\n#     F.when(F.col(\"completion_rate_raw\") > MAX_COMPLETION, MAX_COMPLETION)\n#      .otherwise(F.col(\"completion_rate_raw\"))\n# ).withColumn(\n#     \"unique_videos_ratio\",\n#     F.when(F.col(\"unique_videos_ratio_raw\") > MAX_RATIO, MAX_RATIO)\n#      .otherwise(F.col(\"unique_videos_ratio_raw\"))\n# )\n# # --- KẾT THÚC TINH CHỈNH 2 ---\n\n\n# # Sắp xếp lại các cột (Giữ nguyên)\n# final_columns = [\n#     \"user_id\", \"course_id\", \"certificate\", \"enroll_time\",\n#     \"start_date\", \"end_date\", \"duration_days\", \"remain_day\",\n#     \"first_local_start_time\",\n#     \"days_from_enroll_to_first_watch\",\n#     \"user_video_list\", \"avg_watched_duration\", \"avg_speed\",\n#     \"num_videos_watched\", \"num_sessions\", \"total_watched_duration\",\n#     \"course_video_list\", \"total_video_duration\",\n#     \"num_common_videos\", \"estimated_watched_in_course\", \"completion_rate\", # Đã chuẩn hóa\n#     \"unique_videos_ratio\", # Đã chuẩn hóa\n#     \"rewatch_ratio_original\",\n#     \"rewatch_sessions_ratio\",\n#     \"avg_completion_rate_per_video\", # Đã sửa logic\n#     \"video_activity_ratio\"\n# ]\n# df_final_output = df_final_output.select(final_columns)\n\n\n# # THÊM MỚI: Vòng lặp định dạng % (Thêm vào cuối)\n# rate_cols = [\n#     \"completion_rate\",\n#     \"unique_videos_ratio\",\n#     \"rewatch_ratio_original\",\n#     \"rewatch_sessions_ratio\",\n#     \"avg_completion_rate_per_video\",\n#     \"video_activity_ratio\"\n# ]\n\n# for c in rate_cols:\n#     df_final_output = df_final_output.withColumn(\n#         c,\n#         F.concat(F.round(F.col(c) * 100, 2).cast(\"string\"), F.lit(\"%\"))\n#     )\n\n# # Hiển thị kết quả (đã tinh chỉnh và chuẩn hóa)\n# df_final_output.orderBy(F.col(\"completion_rate\").desc_nulls_last()).show(20, truncate=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Lọc để chỉ hiển thị những hàng có thời gian xem lớn hơn 0\n# df_active_users = df_final_output.filter(F.col(\"total_watched_duration\") > 0)\n\n# df_active_users.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# total = df_final_output.count()\n\n# null_ratio = df_final_output.select([\n#     (F.count(F.when(F.col(c).isNull(), 1)) / total * 100).alias(c) for c in df_final_output.columns\n# ])\n# null_ratio.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from pyspark.sql import functions as F\n\n# # Tạo một DataFrame mới để chuẩn bị lưu CSV\n# # Chuyển đổi các cột mảng thành chuỗi, phân tách bằng dấu '|'\n# df_for_csv = df_final_output.withColumn(\n#     \"user_video_list\", \n#     F.concat_ws(\"|\", F.col(\"user_video_list\"))\n# ).withColumn(\n#     \"course_video_list\", \n#     F.concat_ws(\"|\", F.col(\"course_video_list\"))\n# )\n\n# # Bây giờ tất cả các cột đều là kiểu nguyên thủy, có thể lưu ra CSV\n# df_for_csv.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"/kaggle/working/df_final_output\")\n\n# print(\"Đã lưu file CSV thành công!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import shutil\n\n# input_dir = \"/kaggle/working/df_final_output\"\n# output_zip = \"/kaggle/working/df_final_output\"\n\n# shutil.make_archive(output_zip, 'zip', input_dir)\n# print(\"✅ Đã nén file thành df_final_output.zip\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}